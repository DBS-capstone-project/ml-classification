{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11205113,"sourceType":"datasetVersion","datasetId":6996221}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:51:38.185076Z","iopub.execute_input":"2025-04-05T10:51:38.185292Z","iopub.status.idle":"2025-04-05T10:51:39.068713Z","shell.execute_reply.started":"2025-04-05T10:51:38.185271Z","shell.execute_reply":"2025-04-05T10:51:39.067964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/frankendata/final_fix_V2_merged_emotion_dataset.csv\")\ndata.head","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:51:39.069601Z","iopub.execute_input":"2025-04-05T10:51:39.070075Z","iopub.status.idle":"2025-04-05T10:51:39.219217Z","shell.execute_reply.started":"2025-04-05T10:51:39.070045Z","shell.execute_reply":"2025-04-05T10:51:39.217840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X, y = data['text'], data['label']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:51:39.220892Z","iopub.execute_input":"2025-04-05T10:51:39.221157Z","iopub.status.idle":"2025-04-05T10:51:39.229415Z","shell.execute_reply.started":"2025-04-05T10:51:39.221133Z","shell.execute_reply":"2025-04-05T10:51:39.228486Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, BatchNormalization, GlobalMaxPooling1D, Dense, Dropout, LeakyReLU, ReLU\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.regularizers import l2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nencoder = LabelEncoder()\ny_encoded = encoder.fit_transform(y)\n\ntokenizer = Tokenizer(num_words=30000, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(X)\nX_sequences = tokenizer.texts_to_sequences(X)\nX_padded = pad_sequences(X_sequences, maxlen=100, padding='post')\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_padded, y_encoded, test_size=0.2, random_state=42\n)\n\nmodel = Sequential([\n    Embedding(input_dim=30000, output_dim=256, trainable=True),\n    Conv1D(256, 3, padding='same', kernel_regularizer=l2(0.0005)),\n    BatchNormalization(),\n    ReLU(),\n    Conv1D(128, 5, padding='same', kernel_regularizer=l2(0.0005)),\n    BatchNormalization(),\n    LeakyReLU(alpha=0.1),\n    GlobalMaxPooling1D(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(len(encoder.classes_), activation='softmax')\n])\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.000005),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=2, restore_best_weights=True, mode='max'),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=1, min_lr=1e-6)\n]\n\nhistory = model.fit(\n    X_train, y_train,\n    epochs=50,\n    batch_size=32,\n    callbacks=callbacks\n)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-04-05T10:40:22.082873Z","iopub.execute_input":"2025-04-05T10:40:22.083199Z","iopub.status.idle":"2025-04-05T10:40:26.350117Z","shell.execute_reply.started":"2025-04-05T10:40:22.083174Z","shell.execute_reply":"2025-04-05T10:40:26.348750Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Load and inspect the data\ndata = pd.read_csv(\"/kaggle/input/frankendata/final_fix_V2_merged_emotion_dataset.csv\")\nprint(data.head())\n\n# Extract features and labels\nX, y = data['text'], data['label']\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, LeakyReLU\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport pickle\n\n# Encode labels\nencoder = LabelEncoder()\ny_encoded = encoder.fit_transform(y)\n\n# Tokenize and pad the text data\ntokenizer = Tokenizer(num_words=30000, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(X)\nX_sequences = tokenizer.texts_to_sequences(X)\nX_padded = pad_sequences(X_sequences, maxlen=100, padding='post')\n\n# Save the tokenizer for later inference if needed\nwith open(\"tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(tokenizer, f)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X_padded, y_encoded, test_size=0.2, random_state=42\n)\n\n# Compute class weights to address potential class imbalance\nfrom sklearn.utils import class_weight\nclass_weights_values = class_weight.compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(y_train),\n    y=y_train\n)\nclass_weights = dict(enumerate(class_weights_values))\n\n# Define early stopping callback\nearly_stopper = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=3, restore_best_weights=True\n)\n\n# Build the model using LSTM layers\nmodel = Sequential([\n    # Adding input_length ensures layers are built immediately\n    Embedding(input_dim=30000, output_dim=256, input_length=100),\n    Bidirectional(LSTM(32, recurrent_dropout=0.2)),\n    Dense(16, kernel_regularizer=tf.keras.regularizers.l2(0.006)),\n    Dropout(0.3),\n    LeakyReLU(),\n    Dense(6, activation='softmax')\n])\n\nmodel.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n    metrics=['accuracy']\n)\n\nmodel.summary()\n\n# Train the model\nhistory = model.fit(\n    X_train, \n    y_train, \n    epochs=20, \n    batch_size=32, \n    validation_split=0.2, \n    class_weight=class_weights,\n    callbacks=[early_stopper]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:51:39.231318Z","iopub.execute_input":"2025-04-05T10:51:39.231546Z","iopub.status.idle":"2025-04-05T10:55:46.956225Z","shell.execute_reply.started":"2025-04-05T10:51:39.231524Z","shell.execute_reply":"2025-04-05T10:55:46.955498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(dict(zip(encoder.classes_, encoder.transform(encoder.classes_))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T11:03:07.730632Z","iopub.execute_input":"2025-04-05T11:03:07.730990Z","iopub.status.idle":"2025-04-05T11:03:07.736006Z","shell.execute_reply.started":"2025-04-05T11:03:07.730923Z","shell.execute_reply":"2025-04-05T11:03:07.735103Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\nwith open(\"tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(tokenizer, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:56:28.914070Z","iopub.execute_input":"2025-04-05T10:56:28.914395Z","iopub.status.idle":"2025-04-05T10:56:28.938700Z","shell.execute_reply.started":"2025-04-05T10:56:28.914356Z","shell.execute_reply":"2025-04-05T10:56:28.938050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"emotion_classifier_model_rev2.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T10:56:30.720427Z","iopub.execute_input":"2025-04-05T10:56:30.720715Z","iopub.status.idle":"2025-04-05T10:56:30.873189Z","shell.execute_reply.started":"2025-04-05T10:56:30.720693Z","shell.execute_reply":"2025-04-05T10:56:30.872463Z"}},"outputs":[],"execution_count":null}]}